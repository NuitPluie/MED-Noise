import re
import json
import torch
import string
import numpy as np
from tqdm import tqdm
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from PIL import Image
import os

# å®šä¹‰é¢œè‰²çš„ANSIä»£ç 
RED = '\033[91m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
RESET = '\033[0m'  # é‡ç½®é¢œè‰²

# æ¨¡å‹è·¯å¾„
model_path = "/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/shared/mllm_ckpts/LLaVA-NeXT-Video-7B-hf"

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
try:
    model = LlavaNextForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda:3",  # æ ¹æ®å®é™…GPUç¼–å·ä¿®æ”¹
        trust_remote_code=True
    )
    processor = LlavaNextProcessor.from_pretrained(model_path)
    print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

# æ•°æ®è·¯å¾„
noise_name = 'UE+other'  # ä¿®æ”¹ä¸ºæ‰€éœ€çš„å™ªå£°ç±»å‹ï¼Œä¾‹å¦‚ 'BC+other'
input_data_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/data.json'

# æå–æ–‡ä»¶å¤¹åä½œä¸ºè¾“å‡ºæ–‡ä»¶å
folder_name = os.path.basename(os.path.dirname(input_data_path))  # æå– 'BC'
output_filename = f"LLaVA-NeXT-Video-7B-hf_result/{folder_name}_result.json"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs("LLaVA-NeXT-Video-7B-hf_result", exist_ok=True)

# åŠ è½½æ•°æ®
if not os.path.exists(input_data_path):
    print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_data_path}")
    exit(1)

try:
    with open(input_data_path, 'r') as file:
        wikimultihopqa = json.load(file)
    print(f"æ•°æ®æ ·æœ¬æ•°: {len(wikimultihopqa)}")
except Exception as e:
    print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
    exit(1)

combine_results = []
for item in tqdm(wikimultihopqa[:]):
    print("########################################")
    try:
        # è·å–å›¾åƒè·¯å¾„
        if item['type'][0] == 'crop':
            input_image_path = item['ori_image_path']
        else:
            input_image_path = item['processed_image_path']
        input_image_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/images/' + input_image_path
        
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_image_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {input_image_path}")
            pred_answer = "Image not found"
            combine_results.append({'pred_answer': pred_answer, 'gt': item['answer'], 'query': item['question']})
            continue

        # åŠ è½½å›¾åƒ
        image = Image.open(input_image_path).convert("RGB")
        
        # æ„é€ è¾“å…¥æ–‡æœ¬
        query = item['question']
        input_text = query + '\n' + "Answer the question directly. The answer should be very brief."
        print(RED + input_text + RESET)
        print(GREEN + str(item['answer']) + RESET)
        
        # æ„é€ prompt - LLaVA-Next çš„æ ¼å¼
        prompt = f"<image>\nUSER: {input_text}\nASSISTANT:"
        
        # ğŸ”§ ä½¿ç”¨ LLaVA-Next çš„å¤„ç†æ–¹å¼
        inputs = processor(prompt, image, return_tensors="pt").to(model.device)
        
        # æ¨ç†
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=256,  # é™ä½ç”Ÿæˆé•¿åº¦
                do_sample=False,
                temperature=0.2,
                top_p=0.9,
                use_cache=True
            )
        
        # è§£ç è¾“å‡º
        result = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
        
        # æå–å›ç­”éƒ¨åˆ†
        if "ASSISTANT:" in result:
            pred_answer = result.split("ASSISTANT:")[-1].strip()
        elif "USER:" in result:
            # å¦‚æœåŒ…å«å®Œæ•´å¯¹è¯ï¼Œæå–æœ€åçš„å›ç­”
            parts = result.split("ASSISTANT:")
            if len(parts) > 1:
                pred_answer = parts[-1].strip()
            else:
                pred_answer = result.split("USER:")[-1].strip()
        else:
            pred_answer = result.strip()
        
        # æ¸…ç†å›ç­”ä¸­çš„å¤šä½™å†…å®¹
        if pred_answer.startswith("ASSISTANT:"):
            pred_answer = pred_answer[10:].strip()
        
        print(YELLOW + pred_answer + RESET)

    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        pred_answer = "Error in processing"
    
    combine_results.append(
        {'pred_answer': pred_answer, 'gt': item['answer'], 'query': item['question']}
    )

# ä¿å­˜ç»“æœ
try:
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(combine_results, f, ensure_ascii=False, indent=4)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(combine_results)} ä¸ªæ ·æœ¬")
except Exception as e:
    print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")