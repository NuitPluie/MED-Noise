import re
import json
import torch
import string
import numpy as np
from tqdm import tqdm
from transformers import pipeline
from PIL import Image
import os

# å®šä¹‰é¢œè‰²çš„ANSIä»£ç 
RED = '\033[91m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
RESET = '\033[0m'  # é‡ç½®é¢œè‰²

# ğŸ”§ ä½¿ç”¨pipelineæ–¹å¼åŠ è½½æ¨¡å‹
model_id = "/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/shared/mllm_ckpts/medgemma-4b-pt"

pipe = pipeline(
    "image-text-to-text",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device="cuda:3",  # æ ¹æ®å®é™…GPUç¼–å·ä¿®æ”¹
    trust_remote_code=True
)

noise_name = 'BC'  # ä¿®æ”¹ä¸ºæ‰€éœ€çš„å™ªå£°ç±»å‹ï¼Œä¾‹å¦‚ 'BC+other'

input_data_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/data.json'

# æå–æ–‡ä»¶å¤¹åä½œä¸ºè¾“å‡ºæ–‡ä»¶å
folder_name = os.path.basename(os.path.dirname(input_data_path))  # æå–æ–‡ä»¶å¤¹å
output_filename = f"medgemma-4b-pt_result/{folder_name}_result.json"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs("medgemma-4b-pt_result", exist_ok=True)

with open(input_data_path, 'r') as file:
    wikimultihopqa = json.load(file)
print(f"Loaded {len(wikimultihopqa)} samples")

combine_results = []
for item in tqdm(wikimultihopqa[:]):
    print("########################################")
    
    try:
        # å®‰å…¨è·å–å›¾åƒè·¯å¾„
        if item['type'][0] == 'crop':
            input_image_path = item['ori_image_path']
        else:
            input_image_path = item['processed_image_path']
        
        input_image_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/images/' + input_image_path
        
        query = item['question']
        data_type = item['type']
        item_id = item['id']
        answer = item['answer']

        # ğŸ”§ æŒ‰ç…§pipelineç¤ºä¾‹æ ¼å¼æ„å»ºprompt
        prompt = f"<start_of_image> {query} Answer the question directly. The answer should be very brief."

        print(RED + prompt + RESET)
        print(GREEN + str(answer) + RESET)
        
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_image_path):
            pred_answer = "Image file not found"
            print(f"âš ï¸ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {input_image_path}")
        else:
            # ğŸ”§ æŒ‰ç…§pipelineç¤ºä¾‹åŠ è½½å›¾åƒ
            image = Image.open(input_image_path)
            
            # ğŸ”§ ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†
            output = pipe(
                images=image,
                text=prompt,
                max_new_tokens=256,
                do_sample=True,      # å¯ç”¨é‡‡æ ·
                temperature=0.7,     # æ§åˆ¶éšæœºæ€§
                top_p=0.9,          # nucleusé‡‡æ ·
                top_k=50            # top-ké‡‡æ ·
            )
            
            # ğŸ”§ ä»pipelineè¾“å‡ºä¸­æå–ç”Ÿæˆçš„æ–‡æœ¬
            pred_answer = output[0]["generated_text"].strip()
            
            # ç§»é™¤promptéƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„ç­”æ¡ˆ
            if prompt in pred_answer:
                pred_answer = pred_answer.replace(prompt, "").strip()
            
        print(YELLOW + pred_answer + RESET)

    except Exception as e:
        print("ERROR OCCURS")
        print(f"Error details: {e}")
        import traceback
        traceback.print_exc()
        pred_answer = "Error during processing"
    
    combine_results.append({
        'pred_answer': pred_answer, 
        'gt': answer, 
        'query': query,
        'id': item_id
    })

print(f"Processed {len(combine_results)} samples")
with open(output_filename, "w", encoding="utf-8") as f:
    json.dump(combine_results, f, ensure_ascii=False, indent=4)
    
print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")

# ç®€å•ç»Ÿè®¡
success_count = sum(1 for r in combine_results if r['pred_answer'] not in ["Error during processing", "Image file not found"])
print(f"ğŸ“Š æˆåŠŸå¤„ç†æ ·æœ¬: {success_count}/{len(combine_results)} ({success_count/len(combine_results)*100:.1f}%)")