import re
import json
import torch
import string
import numpy as np
from tqdm import tqdm
from transformers import LlavaProcessor, LlavaForConditionalGeneration, AutoProcessor, AutoModel, AutoTokenizer
from PIL import Image
import os

# å®šä¹‰é¢œè‰²çš„ANSIä»£ç 
RED = '\033[91m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
RESET = '\033[0m'  # é‡ç½®é¢œè‰²

# æ¨¡å‹è·¯å¾„
model_path = "/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/shared/mllm_ckpts/MedM-VL-2D-3B-en"

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
try:
    model = AutoModel.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        device_map="cuda:3",  # æ ¹æ®å®é™…GPUç¼–å·ä¿®æ”¹
    )
    processor = AutoProcessor.from_pretrained(model_path)
    print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

# æ•°æ®è·¯å¾„
noise_name = 'origin'  # ä¿®æ”¹ä¸ºæ‰€éœ€çš„å™ªå£°ç±»å‹ï¼Œä¾‹å¦‚ 'BC+other'
input_data_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/data.json'

# æå–æ–‡ä»¶å¤¹åä½œä¸ºè¾“å‡ºæ–‡ä»¶å
folder_name = os.path.basename(os.path.dirname(input_data_path))  # æå– 'BC'
output_filename = f"MedM-VL-2D-3B-en_result/{folder_name}_result.json"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs("MedM-VL-2D-3B-en_result", exist_ok=True)

# åŠ è½½æ•°æ®
if not os.path.exists(input_data_path):
    print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_data_path}")
    exit(1)

try:
    with open(input_data_path, 'r') as file:
        wikimultihopqa = json.load(file)
    print(f"æ•°æ®æ ·æœ¬æ•°: {len(wikimultihopqa)}")
except Exception as e:
    print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
    exit(1)

combine_results = []
for item in tqdm(wikimultihopqa[:]):
    print("########################################")
    try:
        # è·å–å›¾åƒè·¯å¾„
        if item['type'][0] == 'crop':
            input_image_path = item['ori_image_path']
        else:
            input_image_path = item['processed_image_path']
        input_image_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/images/' + input_image_path
        
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_image_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {input_image_path}")
            pred_answer = "Image not found"
            combine_results.append({'pred_answer': pred_answer, 'gt': item['answer'], 'query': item['question']})
            continue

        # åŠ è½½å›¾åƒ
        image = Image.open(input_image_path).convert("RGB")
        
        # æ„é€ è¾“å…¥æ–‡æœ¬
        query = item['question']
        input_text = query + '\n' + "Answer the question directly. The answer should be very brief."
        print(RED + input_text + RESET)
        print(GREEN + str(item['answer']) + RESET)
        
        # æ„é€ prompt - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
        prompt = f"USER: <image>\n{input_text}\nASSISTANT:"
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„å‚æ•°é¡ºåº - images åœ¨å‰ï¼Œtext åœ¨å
        inputs = processor(images=image, text=prompt, return_tensors="pt").to(model.device)
        
        # æ¨ç†
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=256,  # é™ä½ç”Ÿæˆé•¿åº¦
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
                top_k=50,
                use_cache=True,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id  # æ·»åŠ  pad_token_id
            )
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ tokenizer è€Œä¸æ˜¯ processor è¿›è¡Œè§£ç 
        result = processor.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
        
        # æå–å›ç­”éƒ¨åˆ†
        if "ASSISTANT:" in result:
            pred_answer = result.split("ASSISTANT:")[-1].strip()
        elif "USER:" in result:
            # å¦‚æœåŒ…å«å®Œæ•´å¯¹è¯ï¼Œæå–æœ€åçš„å›ç­”
            parts = result.split("ASSISTANT:")
            if len(parts) > 1:
                pred_answer = parts[-1].strip()
            else:
                pred_answer = result.split("USER:")[-1].strip()
        else:
            pred_answer = result.strip()
        
        print(YELLOW + pred_answer + RESET)

    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        pred_answer = "Error in processing"
    
    combine_results.append(
        {'pred_answer': pred_answer, 'gt': item['answer'], 'query': item['question']}
    )

# ä¿å­˜ç»“æœ
try:
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(combine_results, f, ensure_ascii=False, indent=4)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(combine_results)} ä¸ªæ ·æœ¬")
except Exception as e:
    print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")