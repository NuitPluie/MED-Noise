import re
import json
import torch
import string
import numpy as np
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import os

# å®šä¹‰é¢œè‰²çš„ANSIä»£ç 
RED = '\033[91m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
RESET = '\033[0m'  # é‡ç½®é¢œè‰²

# ğŸ”§ æ ¹æ®å®˜æ–¹ç¤ºä¾‹ä¿®æ”¹æ¨¡å‹IDå’ŒåŠ è½½æ–¹å¼
model_id = "/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/shared/mllm_ckpts/medgemma-4b-pt"

# ğŸ”§ ä½¿ç”¨AutoModelForImageTextToTextæ›¿ä»£Gemma3ForConditionalGeneration
model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="cuda:3",  # æ ¹æ®å®é™…GPUç¼–å·ä¿®æ”¹
    trust_remote_code=True
).eval()

processor = AutoProcessor.from_pretrained(model_id)

noise_name = 'SM'  # ä¿®æ”¹ä¸ºæ‰€éœ€çš„å™ªå£°ç±»å‹ï¼Œä¾‹å¦‚ 'BC+other'

input_data_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/data.json'

# æå–æ–‡ä»¶å¤¹åä½œä¸ºè¾“å‡ºæ–‡ä»¶å
folder_name = os.path.basename(os.path.dirname(input_data_path))  # æå–æ–‡ä»¶å¤¹å
output_filename = f"medgemma-4b-pt_result/{folder_name}_result.json"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs("medgemma-4b-pt_result", exist_ok=True)

with open(input_data_path, 'r') as file:
    wikimultihopqa = json.load(file)
print(f"Loaded {len(wikimultihopqa)} samples")

combine_results = []
for item in tqdm(wikimultihopqa[:]):
    print("########################################")
    
    try:
        # å®‰å…¨è·å–å›¾åƒè·¯å¾„
        if item['type'][0] == 'crop':
            input_image_path = item['ori_image_path']
        else:
            input_image_path = item['processed_image_path']
        
        input_image_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/images/' + input_image_path
        
        query = item['question']
        data_type = item['type']
        item_id = item['id']
        answer = item['answer']

        # ğŸ”§ æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹æ ¼å¼æ„å»ºprompt
        prompt = f"<start_of_image> {query} Answer the question directly. The answer should be very brief, not to explain."

        print(RED + prompt + RESET)
        print(GREEN + str(answer) + RESET)
        
        # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_image_path):
            pred_answer = "Image file not found"
            print(f"âš ï¸ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {input_image_path}")
        else:
            # ğŸ”§ æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹åŠ è½½å›¾åƒ
            image = Image.open(input_image_path).convert("RGB")
            
            # ğŸ”§ æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹å¤„ç†è¾“å…¥
            inputs = processor(
                text=prompt, 
                images=image, 
                return_tensors="pt"
            ).to(model.device, dtype=torch.bfloat16)

            input_len = inputs["input_ids"].shape[-1]

            # ğŸ”§ æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹ç”Ÿæˆå›ç­”
            with torch.inference_mode():
                generation = model.generate(
                    **inputs, 
                    max_new_tokens=256,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                    do_sample=False,      # å¯ä»¥æ”¹ä¸ºFalseä½¿ç”¨è´ªå©ªè§£ç 
                    temperature=0.7,     # åªæœ‰do_sample=Trueæ—¶ç”Ÿæ•ˆ
                    top_p=0.9,          # åªæœ‰do_sample=Trueæ—¶ç”Ÿæ•ˆ
                    top_k=50            # åªæœ‰do_sample=Trueæ—¶ç”Ÿæ•ˆ
                )
                generation = generation[0][input_len:]

            # ğŸ”§ æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹è§£ç è¾“å‡º
            pred_answer = processor.decode(generation, skip_special_tokens=True).strip()
            
        print(YELLOW + pred_answer + RESET)

    except Exception as e:
        print("ERROR OCCURS")
        print(f"Error details: {e}")
        import traceback
        traceback.print_exc()
        pred_answer = "Error during processing"
    
    combine_results.append({
        'pred_answer': pred_answer, 
        'gt': answer, 
        'query': query,
        'id': item_id
    })

print(f"Processed {len(combine_results)} samples")
with open(output_filename, "w", encoding="utf-8") as f:
    json.dump(combine_results, f, ensure_ascii=False, indent=4)
    
print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")

# ç®€å•ç»Ÿè®¡
success_count = sum(1 for r in combine_results if r['pred_answer'] not in ["Error during processing", "Image file not found"])
print(f"ğŸ“Š æˆåŠŸå¤„ç†æ ·æœ¬: {success_count}/{len(combine_results)} ({success_count/len(combine_results)*100:.1f}%)")