import re
import json
import torch
import string
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import pandas as pd

def is_chinese(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å«ä¸­æ–‡å­—ç¬¦"""
    return any('\u4e00' <= ch <= '\u9fff' for ch in text)

def normalize(s):
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        chinese_punc = "ï¼ï¼Ÿï½¡ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½""''ã€ã€‚ï¼šã€Šã€‹ã€ã€‘"
        exclude = set(string.punctuation + chinese_punc)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    s = lower(s)
    s = remove_punc(s)

    if is_chinese(s):
        s = s.replace(" ", "")  # ä¸­æ–‡ä¸€èˆ¬å»é™¤æ‰€æœ‰ç©ºç™½
    else:
        s = remove_articles(s)
        s = white_space_fix(s)

    return s

def compute_f1(prediction, ground_truth):
    if prediction is None:
        return 0.0

    norm_pred = normalize(prediction)
    norm_gt = normalize(ground_truth)

    # ä¸­æ–‡ä½¿ç”¨å­—ç¬¦çº§ï¼Œè‹±æ–‡ä½¿ç”¨è¯çº§
    if is_chinese(norm_pred) or is_chinese(norm_gt):
        pred_tokens = list(norm_pred)
        gt_tokens = list(norm_gt)
    else:
        pred_tokens = norm_pred.split()
        gt_tokens = norm_gt.split()

    common = set(pred_tokens) & set(gt_tokens)
    num_same = len(common)

    if num_same == 0:
        return 0.0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(gt_tokens)
    f1 = 2 * precision * recall / (precision + recall)
    return f1

def exact_match_score(prediction, ground_truth):
    if prediction is None:
        return 0.0
    return int(normalize(prediction) == normalize(ground_truth))

def evaluate_json_file(json_path):
    """è¯„ä¼°å•ä¸ªJSONæ–‡ä»¶"""
    with open(json_path, "r") as f:
        data = json.load(f)
    
    f1_all = []
    em_all = []
    count = 0
    none_count = 0
    
    for item in data:
        if item.get('pred_answer') is not None:
            count += 1
            # è®¡ç®— f1 å’Œ em 
            pred = item['pred_answer']
            gts = item['gt']
            # è‹¥gtæ˜¯strï¼Œç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨å¤„ç†
            if isinstance(gts, str):
                gts = [gts]
            f1 = max([compute_f1(pred, gt) for gt in gts])
            print(f"gt:{gts}, pred: {pred}, f1: {f1}")
            em = max([exact_match_score(pred, gt) for gt in gts])
            if em == 1:
                f1 = 1
            f1_all.append(f1)
            em_all.append(em)
        else:
            count += 1
            none_count += 1
            f1 = 0.0
            em = 0.0
            f1_all.append(f1)
            em_all.append(em)
    
    # è®¡ç®—å¹³å‡å€¼
    avg_f1 = sum(f1_all) / len(f1_all) if f1_all else 0.0
    avg_em = sum(em_all) / len(em_all) if em_all else 0.0
    
    return {
        'file_name': os.path.basename(json_path),
        'total_count': count,
        'none_count': none_count,
        'valid_count': count - none_count,
        'avg_f1': avg_f1,
        'avg_em': avg_em,
        'f1_scores': f1_all,
        'em_scores': em_all
    }

def main():
    # è®¾ç½®ç›®å½•è·¯å¾„
    result_dir = "/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/evaluation_coding/scripts/Mini-InternVL2-4B-DA-Medical_result"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(result_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {result_dir}")
        return
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = [f for f in os.listdir(result_dir) if f.endswith('.json')]
    
    if not json_files:
        print(f"âŒ åœ¨ç›®å½• {result_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶:")
    for f in json_files:
        print(f"  - {f}")
    
    # è¯„ä¼°æ‰€æœ‰æ–‡ä»¶
    all_results = []
    detailed_results = {}
    
    print("\nå¼€å§‹è¯„ä¼°...")
    for json_file in tqdm(json_files, desc="è¯„ä¼°è¿›åº¦"):
        json_path = os.path.join(result_dir, json_file)
        try:
            result = evaluate_json_file(json_path)
            all_results.append(result)
            detailed_results[json_file] = result
            print(f"âœ… {json_file}: F1={result['avg_f1']:.4f}, EM={result['avg_em']:.4f}")
        except Exception as e:
            print(f"âŒ è¯„ä¼° {json_file} æ—¶å‡ºé”™: {e}")
    
    # åˆ›å»ºç»“æœæ±‡æ€»
    summary_results = []
    for result in all_results:
        summary_results.append({
            'File': result['file_name'],
            'Total_Count': result['total_count'],
            'None_Count': result['none_count'],
            'Valid_Count': result['valid_count'],
            'Avg_F1': f"{result['avg_f1']:.4f}",
            'Avg_EM': f"{result['avg_em']:.4f}"
        })
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(summary_results)
    
    # è®¡ç®—æ€»ä½“å¹³å‡å€¼
    total_f1_scores = []
    total_em_scores = []
    for result in all_results:
        total_f1_scores.extend(result['f1_scores'])
        total_em_scores.extend(result['em_scores'])
    
    overall_f1 = sum(total_f1_scores) / len(total_f1_scores) if total_f1_scores else 0.0
    overall_em = sum(total_em_scores) / len(total_em_scores) if total_em_scores else 0.0
    
    # æ·»åŠ æ€»ä½“ç»“æœè¡Œ
    overall_row = {
        'File': 'OVERALL',
        'Total_Count': sum(r['total_count'] for r in all_results),
        'None_Count': sum(r['none_count'] for r in all_results),
        'Valid_Count': sum(r['valid_count'] for r in all_results),
        'Avg_F1': f"{overall_f1:.4f}",
        'Avg_EM': f"{overall_em:.4f}"
    }
    df = pd.concat([df, pd.DataFrame([overall_row])], ignore_index=True)
    
    # ä¿å­˜ç»“æœ
    output_dir = os.path.join(result_dir, "evaluation_results")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜CSVæ ¼å¼çš„æ±‡æ€»ç»“æœ
    csv_path = os.path.join(output_dir, "evaluation_summary.csv")
    df.to_csv(csv_path, index=False)
    print(f"âœ… æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
    
    # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
    detailed_json_path = os.path.join(output_dir, "detailed_evaluation_results.json")
    with open(detailed_json_path, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_json_path}")
    
    # æ‰“å°ç»“æœè¡¨æ ¼
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœæ±‡æ€»:")
    print("="*80)
    print(df.to_string(index=False))
    print("="*80)
    
    # ä¿å­˜æ–‡æœ¬æ ¼å¼çš„ç»“æœ
    txt_path = os.path.join(output_dir, "evaluation_summary.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write("LLaVA-NeXT-Video-7B-hf æ¨¡å‹è¯„ä¼°ç»“æœ\n")
        f.write("="*80 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {pd.Timestamp.now()}\n")
        f.write(f"è¯„ä¼°ç›®å½•: {result_dir}\n")
        f.write(f"æ€»æ–‡ä»¶æ•°: {len(json_files)}\n\n")
        
        f.write("è¯¦ç»†ç»“æœ:\n")
        f.write("-"*80 + "\n")
        f.write(df.to_string(index=False))
        f.write("\n" + "-"*80 + "\n")
        
        f.write(f"\næ€»ä½“æ€§èƒ½:\n")
        f.write(f"  æ•´ä½“å¹³å‡ F1: {overall_f1:.4f}\n")
        f.write(f"  æ•´ä½“å¹³å‡ EM: {overall_em:.4f}\n")
        f.write(f"  æ€»æ ·æœ¬æ•°: {sum(r['total_count'] for r in all_results)}\n")
        f.write(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {sum(r['valid_count'] for r in all_results)}\n")
        f.write(f"  æ— æ•ˆæ ·æœ¬æ•°: {sum(r['none_count'] for r in all_results)}\n")
    
    print(f"âœ… æ–‡æœ¬ç»“æœå·²ä¿å­˜åˆ°: {txt_path}")
    
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“Š æ€»ä½“æ€§èƒ½: F1={overall_f1:.4f}, EM={overall_em:.4f}")

if __name__ == "__main__":
    # å®‰è£…pandaså¦‚æœæ²¡æœ‰
    try:
        import pandas as pd
    except ImportError:
        print("å®‰è£…pandas...")
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pandas"])
        import pandas as pd
    
    main()