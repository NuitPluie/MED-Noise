import re
import json
import torch
import string
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from PIL import Image
import os
import torchvision.transforms as T

# å®šä¹‰é¢œè‰²çš„ANSIä»£ç 
RED = '\033[91m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
BLUE = '\033[94m'
RESET = '\033[0m'

def is_chinese(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å«ä¸­æ–‡å­—ç¬¦"""
    return any('\u4e00' <= ch <= '\u9fff' for ch in text)

def normalize(s):
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        chinese_punc = "ï¼ï¼Ÿï½¡ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½""''ã€ã€‚ï¼šã€Šã€‹ã€ã€‘"
        exclude = set(string.punctuation + chinese_punc)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    s = lower(s)
    s = remove_punc(s)

    if is_chinese(s):
        s = s.replace(" ", "")  # ä¸­æ–‡ä¸€èˆ¬å»é™¤æ‰€æœ‰ç©ºç™½
    else:
        s = remove_articles(s)
        s = white_space_fix(s)

    return s

def compute_f1(prediction, ground_truth):
    if prediction is None:
        return 0.0

    norm_pred = normalize(prediction)
    norm_gt = normalize(ground_truth)

    # ä¸­æ–‡ä½¿ç”¨å­—ç¬¦çº§ï¼Œè‹±æ–‡ä½¿ç”¨è¯çº§
    if is_chinese(norm_pred) or is_chinese(norm_gt):
        pred_tokens = list(norm_pred)
        gt_tokens = list(norm_gt)
    else:
        pred_tokens = norm_pred.split()
        gt_tokens = norm_gt.split()

    common = set(pred_tokens) & set(gt_tokens)
    num_same = len(common)

    if num_same == 0:
        return 0.0

    precision = num_same / len(pred_tokens) if len(pred_tokens) > 0 else 0.0
    recall = num_same / len(gt_tokens) if len(gt_tokens) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    return f1

def exact_match_score(prediction, ground_truth):
    if prediction is None:
        return 0.0
    return int(normalize(prediction) == normalize(ground_truth))

# æ¨¡å‹è·¯å¾„
model_path = "/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/shared/mllm_ckpts/Mini-InternVL2-4B-DA-Medical"

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
try:
    model = AutoModel.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda:3",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    print("âœ… æ¨¡å‹å’ŒtokenizeråŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

# æ•°æ®è·¯å¾„
noise_name = 'origin'  # ä¿®æ”¹ä¸ºæ‰€éœ€çš„å™ªå£°ç±»å‹ï¼Œä¾‹å¦‚ 'BC+other'
input_data_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/data.json'
folder_name = os.path.basename(os.path.dirname(input_data_path))
output_filename = f"Mini-InternVL2-4B-DA-Medical_result/{folder_name}_result.json"
os.makedirs("Mini-InternVL2-4B-DA-Medical_result", exist_ok=True)

# åŠ è½½æ•°æ®
if not os.path.exists(input_data_path):
    print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_data_path}")
    exit(1)

with open(input_data_path, 'r') as file:
    wikimultihopqa = json.load(file)
print(f"æ•°æ®æ ·æœ¬æ•°: {len(wikimultihopqa)}")

# ğŸ”§ å›¾åƒé¢„å¤„ç†å‡½æ•°
def build_transform(input_size):
    MEAN, STD = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=T.InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

# è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

combine_results = []
f1_scores = []
em_scores = []

for idx, item in enumerate(tqdm(wikimultihopqa[:])):
    print(f"\n{'='*80}")
    print(f"æ ·æœ¬ {idx+1}/{len(wikimultihopqa)}")
    print(f"{'='*80}")
    
    try:
        # è·å–å›¾åƒè·¯å¾„
        if item['type'][0] == 'crop':
            input_image_path = item['ori_image_path']
        else:
            input_image_path = item['processed_image_path']
        input_image_path = f'/cluster/home/user1/YuanWenzhen/workspace/Visual-RFT/Visual-ARFT/data/MAT-Benchmark/all_noise_100_test/{noise_name}/images/' + input_image_path
        
        if not os.path.exists(input_image_path):
            pred_answer = "Image not found"
            combine_results.append({'pred_answer': pred_answer, 'gt': item['answer'], 'query': item['question']})
            continue

        # ğŸ”§ æ­£ç¡®çš„å›¾åƒåŠ è½½å’Œå¤„ç†
        image = Image.open(input_image_path).convert("RGB")
        query = item['question']
        input_text = query + '\n' + "Answer the question directly. The answer should be very brief."
        print(RED + f"é—®é¢˜: {input_text}" + RESET)
        print(GREEN + f"æ ‡å‡†ç­”æ¡ˆ: {str(item['answer'])}" + RESET)
        
        # ğŸ”§ ä¿®å¤InternVLçš„æ¨ç†æ–¹å¼
        try:
            generation_config = dict(
                num_beams=1, 
                max_new_tokens=256, 
                do_sample=True,      # ğŸ”§ å¯ç”¨é‡‡æ ·
                temperature=0.7,     # ğŸ”§ è®¾ç½®æ¸©åº¦å‚æ•°
                top_p=0.9,           # ğŸ”§ ä½¿ç”¨top-pé‡‡æ ·
                top_k=50,            # ğŸ”§ æ·»åŠ top-ké‡‡æ ·
                repetition_penalty=1.1  # ğŸ”§ é¿å…é‡å¤
            )
            
            # InternVL2.5éœ€è¦å°†å›¾åƒè½¬æ¢ä¸ºtensor
            transform = build_transform(input_size=448)
            pixel_values = transform(image).unsqueeze(0).to(model.device, dtype=torch.bfloat16)
            
            # ä½¿ç”¨æ­£ç¡®çš„chatæ–¹æ³•
            response = model.chat(
                tokenizer=tokenizer, 
                pixel_values=pixel_values,  # ğŸ”§ ä½¿ç”¨å¤„ç†åçš„tensorè€Œä¸æ˜¯PILå›¾åƒ
                question=input_text, 
                generation_config=generation_config
            )
            pred_answer = response
            
        except Exception as inner_e:
            print(f"Chatæ–¹æ³•å¤±è´¥: {inner_e}")
            try:
                # æ–¹æ³•2: ä½¿ç”¨generateæ–¹æ³•
                transform = build_transform(input_size=448)
                pixel_values = transform(image).unsqueeze(0).to(model.device, dtype=torch.bfloat16)
                
                # æ„é€ è¾“å…¥æ–‡æœ¬
                prompt = f"<image>\nUser: {input_text}\nAssistant:"
                input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        input_ids=input_ids,
                        pixel_values=pixel_values,
                        max_new_tokens=256,
                        num_beams=1,
                        do_sample=False,
                        temperature=0.2,
                        pad_token_id=tokenizer.eos_token_id if hasattr(tokenizer, 'eos_token_id') else tokenizer.pad_token_id
                    )
                
                # åªè§£ç æ–°ç”Ÿæˆçš„token
                new_tokens = output_ids[0][input_ids.shape[1]:]
                pred_answer = tokenizer.decode(new_tokens, skip_special_tokens=True)
                    
            except Exception as inner_e2:
                print(f"Generateæ–¹æ³•ä¹Ÿå¤±è´¥: {inner_e2}")
                pred_answer = "Error in processing"
        
        # æ¸…ç†å›ç­”
        pred_answer = str(pred_answer).strip()
        if pred_answer.startswith("Assistant:"):
            pred_answer = pred_answer[10:].strip()
        if pred_answer.startswith("assistant:"):
            pred_answer = pred_answer[10:].strip()
        if pred_answer.startswith("User:"):
            pred_answer = pred_answer[5:].strip()
        if pred_answer.startswith("user:"):
            pred_answer = pred_answer[5:].strip()
        
        print(YELLOW + f"æ¨¡å‹å›ç­”: {pred_answer}" + RESET)
        
        # ğŸ”§ è®¡ç®—è¯„ä¼°åˆ†æ•°
        gts = item['answer']
        if isinstance(gts, str):
            gts = [gts]
        
        # è®¡ç®—ä¸æ‰€æœ‰ground truthçš„æœ€å¤§åˆ†æ•°
        f1_score = max([compute_f1(pred_answer, gt) for gt in gts])
        em_score = max([exact_match_score(pred_answer, gt) for gt in gts])
        
        # å¦‚æœEM=1ï¼Œåˆ™F1ä¹Ÿè®¾ä¸º1
        if em_score == 1:
            f1_score = 1.0
        
        f1_scores.append(f1_score)
        em_scores.append(em_score)
        
        print(BLUE + f"F1 åˆ†æ•°: {f1_score:.4f}" + RESET)
        print(BLUE + f"EM åˆ†æ•°: {em_score:.4f}" + RESET)

    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        pred_answer = "Error in processing"
        
        # å¯¹äºé”™è¯¯çš„æƒ…å†µä¹Ÿè¦è®¡ç®—åˆ†æ•°
        gts = item['answer']
        if isinstance(gts, str):
            gts = [gts]
        f1_score = 0.0
        em_score = 0.0
        f1_scores.append(f1_score)
        em_scores.append(em_score)
    
    combine_results.append(
        {'pred_answer': pred_answer, 'gt': item['answer'], 'query': item['question']}
    )

# è®¡ç®—æ€»ä½“ç»Ÿè®¡
avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
avg_em = sum(em_scores) / len(em_scores) if em_scores else 0.0

print(f"\n{'='*80}")
print("è¯„ä¼°ç»“æœæ±‡æ€»:")
print(f"{'='*80}")
print(f"æ€»æ ·æœ¬æ•°: {len(wikimultihopqa)}")
print(f"å¹³å‡ F1 åˆ†æ•°: {avg_f1:.4f}")
print(f"å¹³å‡ EM åˆ†æ•°: {avg_em:.4f}")
print(f"{'='*80}")

# åªä¿å­˜åŸå§‹ç»“æœ
with open(output_filename, "w", encoding="utf-8") as f:
    json.dump(combine_results, f, ensure_ascii=False, indent=4)
print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")
print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(combine_results)} ä¸ªæ ·æœ¬")